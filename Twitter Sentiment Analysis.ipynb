{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to use python for determining the sentiment of the given Twitter post. The text can be of following major category- positive, negative, neutral.\n",
    "\n",
    "Following questions will be answered at the end:\n",
    "\n",
    "* What is the ratio of positive to negative words for the given trending topic?\n",
    "* What is my interpretation of the ratio?\n",
    "* What is the managerial insight that can be offered based on the results?\n",
    "\n",
    "To perform the analysis will use following python libraries:\n",
    "\n",
    "* tweepy, to access twitter API\n",
    "* Stanford Natural Language Toolkit, for natural languages functionalities\n",
    "* re, for Regular Expression\n",
    "* string, to perform string manipulations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter API configuration and tweet extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import codecs\n",
    "import credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authenicating user by passing credential keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_setup():\n",
    "    auth = tweepy.OAuthHandler(credentials.CONSUMER_KEY, credentials.CONSUMER_SECRET)\n",
    "    auth.set_access_token(credentials.ACCESS_TOKEN, credentials.ACCESS_SECRET)\n",
    "    api = tweepy.API(auth)\n",
    "    return api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting tweets for the keyword='Trump'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tweets before 866641990716465151\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 783314566386110463\n",
      "...479 tweets downloaded so far\n",
      "getting tweets before 765283905062727679\n",
      "...479 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "api=twitter_setup()\n",
    "alltweets = []\n",
    "#max limit to extract tweet is 200 per request\n",
    "new_tweets = api.user_timeline(screen_name = \"Trump\",count=200)\n",
    "\n",
    "alltweets.extend(new_tweets)\n",
    "\n",
    "oldest = alltweets[-1].id - 1\n",
    "   \n",
    "#keep grabbing tweets until there are no tweets left to grab\n",
    "while len(new_tweets) > 0:\n",
    "    print(\"getting tweets before %s\" % (oldest))\n",
    "\n",
    "    #all subsiquent requests use the max_id param to prevent duplicates\n",
    "    new_tweets = api.user_timeline(screen_name = \"Trump\",count=200,max_id=oldest)\n",
    "\n",
    "    #save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "        \n",
    "    #update the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "\n",
    "    print(\"...%s tweets downloaded so far\" % (len(alltweets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing tweets in a text file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = codecs.open(\"TrumpTweets.txt\", \"w\", \"utf-8\")\n",
    "for tweet in alltweets:\n",
    "    file.write(tweet.text)\n",
    "    file.write(\"\\n\")\n",
    "    \n",
    "file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we have generated a file containing tweets for the further analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing packages\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the tweets using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    #Convert to lower case\n",
    "    tweets = tweet.lower()\n",
    "    #Convert www.* or https?://* or  http?://* to URL\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+)|(https:/[^\\s]+)|(http?[^\\s]+))','URL',tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+',\"AT_USER\",tweet)\n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #Replace all non alphanumeric i.e remove all special characters\n",
    "    tweet=re.sub(r'[^\\w]', ' ', tweet)\n",
    "    #trim\n",
    "    tweet = tweet.strip('\\'\"')\n",
    "    #tweet= tweet.strip()\n",
    "    tweet_list=tweet.split()\n",
    "    return tweet_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making a list of Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a list of stop words from the imported list of stopwords\n",
    "\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "#append url and at_user as stop words\n",
    "def stop_word_list(stop_words):\n",
    "    stop_words.append('AT_USER')\n",
    "    stop_words.append('URL')\n",
    "    stop_words.append('RT')\n",
    "    stop_words.append('a')\n",
    "    return stop_words\n",
    "\n",
    "# add these stop words to stopword list\n",
    "stop_words= stop_word_list(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uploading file of positive and negative list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing positive and negative set of words\n",
    "\n",
    "positive_tweets=open('PositiveWords.txt','r').read().split(\"\\n\")\n",
    "\n",
    "negative_tweets=open('NegativeWords.txt','r').read().split(\"\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign sentiment to each word in the list of cleaned tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_sentiment(final_processed_tweet):\n",
    "    count_sentiment=0\n",
    "    for words in final_processed_tweet:\n",
    "        if words in positive_tweets:\n",
    "            count_sentiment=count_sentiment+1\n",
    "        if words in negative_tweets:\n",
    "            count_sentiment=count_sentiment-1\n",
    "    return count_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determining sentiment by uploading the text file containing 500 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment is positive\n",
      "305\n"
     ]
    }
   ],
   "source": [
    "final_processed_tweet=[]\n",
    "processed_tweet=[]\n",
    "#Read the file line by line\n",
    "fp = open('TrumpTweets.txt', 'r',encoding=\"utf8\")\n",
    "line = fp.readline()\n",
    "while line:\n",
    "    line = fp.readline()\n",
    "\n",
    "    #Add the cleaned tweets to list\n",
    "    clean_tweets=clean_tweet(line)\n",
    "    \n",
    "    #removing stopwords from the clean_tweets list\n",
    "    for i in clean_tweets:\n",
    "        if i in stop_words:\n",
    "            continue\n",
    "        else:\n",
    "            processed_tweet.append(i) \n",
    "            \n",
    "final_processed_tweet.extend(processed_tweet) \n",
    "\n",
    "#Add sentiment to words for each tweet\n",
    "count=determine_sentiment(final_processed_tweet)\n",
    "if count > 0:\n",
    "    print('The sentiment is positive')\n",
    "elif count <0:\n",
    "    print('The sentiment is negative')\n",
    "else:\n",
    "    print('The sentiment is neutral')\n",
    "\n",
    "\n",
    "print(count)\n",
    "#end loop\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Positive and Negative words from the Cleaned tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive words are 332\n",
      "Negative words are 27\n"
     ]
    }
   ],
   "source": [
    "Positive_word=0\n",
    "Negative_word=0\n",
    "for word in final_processed_tweet:\n",
    "    if word not in stop_words:\n",
    "        if word in positive_tweets:\n",
    "            Positive_word+=1\n",
    "        elif word in negative_tweets:\n",
    "            Negative_word+=1\n",
    "print('Positive words are %s' %Positive_word)\n",
    "print('Negative words are %s' %Negative_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ratio of Positive and Negative words in Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of Positive words to Negative words is: 12.296296296296296\n"
     ]
    }
   ],
   "source": [
    "Ratio=Positive_word/Negative_word\n",
    "print('Ratio of Positive words to Negative words is:',Ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis on Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the twitter sentiment for the given topic is Positive\n",
    "\n",
    "The ratio of positive to negative words is also greater than 1 which tells that the number of positive words are more in the given twitter sample\n",
    "\n",
    "Managerial Insights: If anyone is publically associating with the keyword='trump' in social media then it is positively received"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
